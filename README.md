# container-log-management
Docker container log management system using Kafka

### Prerequisites
- Docker
- MongoDB Compass

### Setup
- Create Docker network

`docker network create --bridge kafka`
-  Running the Kafka Broker
    - The Kafka broker container installs and runs zookeeper and kafka.
    - Build Kafka Broker Image:
        
        `docker build -t server_zookeeper -f ./Dockerfile .`
    - Run the Kafka Broker Container

    ` docker run -itd --name server --network kafka server_zookeeper`

    - Check if container is running

    `docker container ls`
    
    You should see a container called `server` running.

- Running the Consumer
    - The consumer container consumes the messages which the producer is streaming.
    - The consumer installs and runs mongodb where all the consumed messages are being stored.
    - Mongodb is being exposed to the host system on port 2717.
    - The stored logs can be viewed using mongodb compass with `root` as the username and `1234` as the password.
    - Build Consumer Image
        
        `docker build -t consumer  -f ./Dockerfile_Consumer .`
    - Run the consumer container
        
        `% docker run -itd -v <PATH to Mongo Volume>:/data/db -p 2717:27017 --name consumer --network kafka consumer`
    - Running consumer.py

        - Exec into the container using: `docker exec -it consumer bash`

        - Run `python3 /home/consumer.py <TOPIC_NAME>`. Now the consumer is ready to consume messages from `<TOPIC_NAME>` topic.
    
- Running the Producer [Single]
    - The producer container runs test.py which simulates 3 applications running on the container that generate logs.
    - These logs are stored in specific locations which are mentioned in `priority.json`.
    - 'priority.json` contians the locations of the log files and the priority in which they should be consumed.
    - `priority.json` can be changed to handle your own application logs. For testing purposes they point to the logs being generated by `test.py`.
    - Build Producer Image

    `docker build -t producer  -f Dockerfile_Producer .`

    - Run the producer container
    
    `docker run -itd --name producer --network kafka producer`

    - Running watcher.py
        - Exec into the container using: `docker exec -it producer bash`

        - Run `python3 /home/watcher.py`. This streams messages from the log files mentioned in `priority.json`. `watcher.py` also streams system logs which are being generated in `/var/log/failog`, `/var/log/apt/history.log`
